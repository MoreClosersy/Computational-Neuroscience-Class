{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reinforcement.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP/C5gIlowid3Maa707YTVp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xup5/Computational-Neuroscience-Class/blob/main/Lab%208%20Reinforcement%20Learning/Reinforcement.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reinforcement Learning\n",
        "\n",
        "by Odelia Schwartz, transcribed to python by Xu Pan, 2022."
      ],
      "metadata": {
        "id": "PcXEc-xn8Kww"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Rescora Wagner model\n",
        "After figure 9.1 in Dayan and Abbott textbook\n",
        "\n",
        "Pairing stimulus u with reward r"
      ],
      "metadata": {
        "id": "OJwLV2D08ePK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01dMvZyB7_se"
      },
      "outputs": [],
      "source": [
        "# After figure 9.1 in Dayan and Abbott textbook\n",
        "# Pairing stimulus u with reward r\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "epsilon = .05   # epsilon\n",
        "w = 0\n",
        "wVec = [w]\n",
        "v = w\n",
        "for i in range(200):\n",
        "  u = 1\n",
        "  if i<100:   # stimulus paired with reward\n",
        "     r = 1\n",
        "  else:\n",
        "     r = 0   # stimulus not paired with reward\n",
        "  w = w + epsilon * (r-v) * u\n",
        "  v = w\n",
        "  wVec.append(w)\n",
        "wVec = np.array(wVec)\n",
        "\n",
        "plt.plot(wVec, 'ko')\n",
        "plt.xlabel('trial number', fontsize=16)\n",
        "plt.ylabel('w', fontsize=16)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Write a version of the above that for each\n",
        "# stimulus has 50 percent chance of being\n",
        "# paired with a reward. Call the resulting\n",
        "# weight vector wVec2 and plot it alongside\n",
        "# wVec above.  \n",
        "\n",
        "w = 0\n",
        "wVec2 = [w]\n",
        "v = w\n",
        "for i in range(200):\n",
        "  u = 1\n",
        "  therand = np.random.rand()\n",
        "  if therand >= .5:\n",
        "     r = 1\n",
        "  else:\n",
        "     r = 0\n",
        "  w = w + epsilon * (r-v) * u\n",
        "  v = w\n",
        "  wVec2.append(w)\n",
        "\n",
        "plt.plot(wVec, 'ko')\n",
        "plt.plot(wVec2, 'sb', markerfacecolor='none')\n",
        "plt.xlabel('trial number', fontsize=16)\n",
        "plt.ylabel('w', fontsize=16)"
      ],
      "metadata": {
        "id": "48CugI-d_UWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**To Do:**\n",
        "Try different learning rates (epsilon values)"
      ],
      "metadata": {
        "id": "eFWkHLprAtvU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Temporal Difference (TD) learning.\n",
        "After figure 9.2 in Dayan and Abbott."
      ],
      "metadata": {
        "id": "DpMO_JqiDAn9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epsilon = .2      # learning rate\n",
        "tStim = 100       # time of stimulus\n",
        "timeSteps = 250   # time steps per trial\n",
        "numTrials = 1000  # number of trials\n",
        "\n",
        "u = np.zeros(timeSteps)  # stimulus\n",
        "r = np.zeros(timeSteps)  # reward\n",
        "v = np.zeros(timeSteps)  # expected future reward\n",
        "u[tStim] = 1\n",
        "r[198] = .5\n",
        "r[199] = 1\n",
        "r[201] = .5\n",
        "\n",
        "# if 0:\n",
        "#   r[180:219]= np.ones(r[180:219].shape)\n",
        "\n",
        "delta = np.zeros((timeSteps,numTrials))  # prediction error over trials and time steps\n",
        "vMat = np.zeros((timeSteps,numTrials))   # expected future reward over trials and time steps\n",
        "vDiff = np.zeros((timeSteps,numTrials))\n",
        "\n",
        "for trial in range(numTrials):\n",
        "  for t in range(1,timeSteps):\n",
        "     delta[t-1,trial] = r[t-1] + v[t] - v[t-1]       # at time t, update the previous\n",
        "     if t>=(tStim-1):                                # update when stimulus is present\n",
        "        v[t-1] = v[t-1] + epsilon*(delta[t-1,trial]) # v expected future reward is updating every trial\n",
        "     vMat[t-1,trial] = v[t-1]\n",
        "     vDiff[t-1,trial] = v[t]-v[t-1]\n",
        "\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure(data=[go.Surface(z=delta, colorscale='Rainbow')])\n",
        "\n",
        "fig.update_layout(scene = dict(\n",
        "                    xaxis_title='trials',\n",
        "                    yaxis_title='t',\n",
        "                    zaxis_title='delta'),\n",
        "                  autosize=False,\n",
        "                  width=600, height=600,\n",
        "                  margin=dict(l=65, r=50, b=65, t=90))\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "4kLkKG2t_XSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**To Do:** How does delta change over trials? Take slices at\n",
        " early trials and late trials, and plot the vectors.\n",
        " What are the changes over time signifying in terms of what is learned?\n",
        " What happens when you change r -- for instance to have uniform reward values\n",
        " between 181:220, or to just have one value at time 150?\n",
        " What happens when you change the time of the stimulus?\n",
        " How does the expected future reward (v) change over trials?\n",
        " You can define a matrix vMat = np.zeros((timeSteps,numTrials))\n",
        " and update it every trial, and then look at slices through\n",
        " the matrix. What are the changes in v over time signifying in terms of what is learned?"
      ],
      "metadata": {
        "id": "V8wMkUYvUF5Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here:\n"
      ],
      "metadata": {
        "id": "TTCyolF1xDbJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}