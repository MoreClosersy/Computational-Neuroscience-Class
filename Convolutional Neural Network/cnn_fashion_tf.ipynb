{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMlwRS0aSw3rKKnK91U8+lm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xup5/Computational-Neuroscience-Class/blob/main/Convolutional%20Neural%20Network/cnn_fashion_tf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Convolutional neural network (CNN) for Fashion MNIST dataset from scratch**\n",
        "\n",
        "\n",
        "by Xu Pan\n",
        "\n",
        "Change colab runtime type to GPU to accelerate: \"Runtime\" --> \"Change runtime type\"--> under \"Hardware accelerator\" select \"GPU\".\n",
        "\n",
        "\n",
        "We will implement a samilar CNN model you have already implemented with Keras. We will try to use generic TF functions as much as possible rather than high Keras encapsulation. Hopefully this will help you understand every details of deep neural network.\n",
        "\n",
        "\n",
        "\n",
        "## **Common deep learning steps**\n",
        "\n",
        "* Step 1: Generate training and test data (and preprocess)\n",
        "* Step 2: Initialize the network parameters\n",
        "* Step 3: Forward propagation\n",
        "* Step 4: Compute the cost/loss\n",
        "* Step 5: Backpropagation or create an optimizer to minimize the cost from step 4\n",
        "* Step 6: Evaluate the model with your test set"
      ],
      "metadata": {
        "id": "BLvWUs5XB5KS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "XWzJ1lExB3li"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "import os   # to save the checkpoint\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load dataset\n",
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "# check shape\n",
        "print('Training data size:' + str(x_train.shape))\n",
        "print('Training labels size: ' + str(y_train.shape))\n",
        "\n",
        "print('Test data size: ' + str(x_test.shape))\n",
        "print('Test label size: ' + str(y_test.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQaPEiGODJMJ",
        "outputId": "5c4e7ed3-f130-446f-b536-c952142eba39"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data size:(60000, 28, 28)\n",
            "Training labels size: (60000,)\n",
            "Test data size: (10000, 28, 28)\n",
            "Test label size: (10000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# normalize input range\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "# Add a channels dimension\n",
        "x_train = x_train[..., np.newaxis].astype(\"float\")\n",
        "x_test = x_test[..., np.newaxis].astype(\"float\")\n",
        "\n",
        "# Change target from categorical to one-hot\n",
        "y_train_one_hot = np.zeros((y_train.size, y_train.max() + 1))\n",
        "y_train_one_hot[np.arange(y_train.size), y_train] = 1\n",
        "y_test_one_hot = np.zeros((y_test.size, y_test.max() + 1))\n",
        "y_test_one_hot[np.arange(y_test.size), y_test] = 1"
      ],
      "metadata": {
        "id": "flggj55jDTzE"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tensorflow dataset class is useful in many cases. Though we are not using it here.\n",
        "\n",
        "# train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train_one_hot)).shuffle(60000).batch(32)\n",
        "# test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test_one_hot)).batch(32)"
      ],
      "metadata": {
        "id": "cKhv-4IbEIXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# keras.Model is a useful model interface. We can define our model computations inside\n",
        "# the call function.\n",
        "\n",
        "# class MyModel(Model):\n",
        "#   def __init__(self):\n",
        "#     super(MyModel, self).__init__()\n",
        "#     self.conv1 = Conv2D(32, 3, activation='relu')\n",
        "#     self.flatten = Flatten()\n",
        "#     self.d1 = Dense(128, activation='relu')\n",
        "#     self.d2 = Dense(10)\n",
        "\n",
        "#   def call(self, x):\n",
        "#     x = self.conv1(x)\n",
        "#     x = self.flatten(x)\n",
        "#     x = self.d1(x)\n",
        "#     return self.d2(x)"
      ],
      "metadata": {
        "id": "G2RheBPuPA_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel():\n",
        "  def __init__(self):\n",
        "    self.w1 = tf.Variable(np.random.normal(0, 0.1, size=(3,3,1,64))) # [filter_height, filter_width, in_channels, out_channels]\n",
        "    self.w2 = tf.Variable(np.random.normal(0, 0.1, size=(3,3,64,32)))\n",
        "    self.w3 = tf.Variable(np.random.normal(0, 0.1, size=(1152,128)))\n",
        "    self.w4 = tf.Variable(np.random.normal(0, 0.1, size=(128,10)))\n",
        "    self.b1 = tf.Variable(np.zeros(64))\n",
        "    self.b2 = tf.Variable(np.zeros(32))\n",
        "\n",
        "  def call(self, x):\n",
        "    x = tf.nn.conv2d(input=x, filters=self.w1, strides=[1, 1, 1, 1], padding='VALID') # b x 26 x 26 x 64\n",
        "    x = tf.nn.bias_add(x, self.b1)\n",
        "    x = tf.nn.relu(x)\n",
        "    x = tf.nn.max_pool(x, ksize=2, strides=2, padding=\"SAME\")\n",
        "\n",
        "    x = tf.nn.conv2d(input=x, filters=self.w2, strides=[1, 1, 1, 1], padding='VALID') # b x 12 x 12 x 32\n",
        "    x = tf.nn.bias_add(x, self.b2)\n",
        "    x = tf.nn.relu(x)\n",
        "    x = tf.nn.max_pool(x, ksize=2, strides=2, padding=\"SAME\")\n",
        "\n",
        "    x = tf.reshape(x, [x.shape[0], 1, -1]) # b x 1 x 1152\n",
        "    x = tf.linalg.matmul(x,self.w3) # b x 1 x 32\n",
        "    x = tf.nn.relu(x)\n",
        "\n",
        "    x = tf.linalg.matmul(x,self.w4) # b x 1 x 10\n",
        "    x = tf.reshape(x, [x.shape[0], -1]) # b x 1\n",
        "    x = tf.nn.softmax(x)\n",
        "    return x\n",
        "  \n",
        "  def get_var(self):\n",
        "    return [self.w1, self.w2, self.w3, self.w4, self.b1, self.b2]\n",
        "\n",
        "# Create an instance of the model\n",
        "model = MyModel()"
      ],
      "metadata": {
        "id": "mO4YnYSjI8qW"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_epoch = 10\n",
        "batch_size = 32\n",
        "max_step_train = int(60000 / batch_size)\n",
        "max_step_test = int(10000 / batch_size)\n",
        "lr = 0.0001\n",
        "# optimizer = tf.keras.optimizers.Adam()\n",
        "for epoch in range(max_epoch):\n",
        "  # train\n",
        "  for step in tqdm(range(max_step_train)):\n",
        "    with tf.GradientTape() as tape:\n",
        "      # training=True is only needed if there are layers with different\n",
        "      # behavior during training versus inference (e.g. Dropout).\n",
        "      predictions = model.call(x_train[step*batch_size:(step+1)*batch_size, ...])\n",
        "      loss = tf.nn.softmax_cross_entropy_with_logits(y_train_one_hot[step*batch_size:(step+1)*batch_size, ...], predictions)\n",
        "    gradients = tape.gradient(loss, model.get_var())\n",
        "    # optimizer.apply_gradients(zip(gradients, model.get_var()))\n",
        "    for var, grad in zip(model.get_var(), gradients):\n",
        "      var.assign_sub(lr*grad)\n",
        "  \n",
        "  # test\n",
        "  test_accuracy = 0\n",
        "  for step in range(max_step_test):\n",
        "    predictions = model.call(x_test[step*batch_size:(step+1)*batch_size, ...])\n",
        "    test_accuracy += np.sum(np.argmax(predictions,axis=1) == y_test[step*batch_size:(step+1)*batch_size, ...])\n",
        "  test_accuracy = test_accuracy/10000\n",
        "  print(f'test accuracy: {test_accuracy}.')"
      ],
      "metadata": {
        "id": "oXvp_VS8TvNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Questions\n",
        "\n",
        "1.  Why do we need to randomly initialize weights?\n",
        "\n",
        "2.  Print out the training accuracy and loss during the training (hint: moving average)\n",
        "\n",
        "3.  What you can do to get higher accuracy?\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SiqR3_MOeUay"
      }
    }
  ]
}